{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network Training for Genes\n",
    "This notebook describes the usage of the __geneGAN_train.py__ script to train a GAN for gene similarity measures or connection discovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from geneGAN_train import geneGAN\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "__dataPath__: Path to GAN training matrix with ixj entries, i being the number of training sampels and j being the number of genes used in training. For gene similarity, each row should have to two entries set to one (corresponding to co-occurring genes), while the rest are zero.\n",
    "\n",
    "__autoencoderPath__: Path to autoencoder training matrix which includes all possible gene pairs used for pretraining the autoencoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataPath = 'data/cosmic_patients_min_50_in.npy'\n",
    "autencoderPath = 'data/all_comb_two_hot.npy'\n",
    "data = np.load(dataPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Setting\n",
    "\n",
    "### GAN Architecture in terms of feature vector dimension:\n",
    "\n",
    "__Autoencoder__: inputDim x [compressor_size] x embed_size x [decompressor_size] x inputDim\n",
    "\n",
    "__Generator__: noise_size x [generator_size] x embed_size\n",
    "\n",
    "__Generator + Decoder__: noise_size x [generator_size] x embed_size x [decompressor_size] x inputDim\n",
    "\n",
    "__Discriminator__: inputDim x [discriminator_size]\n",
    "\n",
    "### Other Settings\n",
    "\n",
    "__datatype__: if 'binary', last layer of decoder uses tanh to map to [0,1], otherwise ReLu.\n",
    "\n",
    "__bnDecay__: batch normalization decay\n",
    "\n",
    "__l2scale__: weight of L2 regularization loss\n",
    "\n",
    "__modelPath__: points to existing model for continuation of training. If '', then a new training is started.\n",
    "\n",
    "__outPath__: points to folder, where the model and results will be saved. 'training_test' is the folder and 'test' is the start of the filenames. All files will be named 'test...'.\n",
    "\n",
    "__pretrainEpochs__: number of autoencoder pretrainingEpochs\n",
    "\n",
    "__nEpochs__: number of main GAN training epochs\n",
    "\n",
    "__discriminatorTrainPeriod__: number of discriminator trainings at each training step, used to set D/G training ratio.\n",
    "\n",
    "__generatorTrainPeriod__: number of generator trainings at each training step, used to set D/G training ratio.\n",
    "\n",
    "__pretrainBatchSize__: Autoencoder pretraining batch sizes.\n",
    "\n",
    "__batchSize__: Main training batchSizes.\n",
    "\n",
    "__saveMaxKeep__: maximum number of intermediate models saved.\n",
    "\n",
    "__keepProb__: Dropout keep probabitity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:0, trainLoss:20.905212, validLoss:11.789073, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:1, trainLoss:7.404072, validLoss:2.667455, validReverseLoss:0.000000\n",
      "Epoch:00000, time:43.19 d_loss:0.97, g_loss:22.59, acc:1.00, score_t:0.99, score_v:0.99, gen_v:0\n",
      "Epoch:00001, time:36.33 d_loss:0.10, g_loss:14.92, acc:1.00, score_t:1.00, score_v:1.00, gen_v:0\n",
      "INFO:tensorflow:trainings/training_test/test is not in all_model_checkpoint_paths. Manually adding it.\n",
      "trainings/training_test/test\n",
      "best epoch scaled: 0\n",
      "best epoch unscaled: 0\n"
     ]
    }
   ],
   "source": [
    "inputDim = data.shape[1]\n",
    "embed_size = 256\n",
    "noise_size = 256\n",
    "generator_size = [256, 256]\n",
    "discriminator_size = [256, 128, 1]\n",
    "compressor_size = []\n",
    "decompressor_size = []\n",
    "\n",
    "ggan = geneGAN(dataType='binary',\n",
    "            inputDim=inputDim,\n",
    "            embeddingDim=embed_size,\n",
    "            randomDim=noise_size,\n",
    "            generatorDims=generator_size,\n",
    "            discriminatorDims=discriminator_size,\n",
    "            compressDims=compressor_size,\n",
    "            decompressDims=decompressor_size,\n",
    "            bnDecay=0.99,\n",
    "            l2scale=0.001)\n",
    "\n",
    "ggan.train(dataPath=dataPath,\n",
    "           autoencoderData=autencoderPath,\n",
    "           modelPath='',\n",
    "           outPath='trainings/training_test/test',\n",
    "           pretrainEpochs=2, #100,\n",
    "           nEpochs=2, #1000,\n",
    "           discriminatorTrainPeriod=2,\n",
    "           generatorTrainPeriod=1,\n",
    "           pretrainBatchSize=100,\n",
    "           batchSize=1000,\n",
    "           saveMaxKeep=0,\n",
    "           keepProb=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Outputs\n",
    "### test_training_stats.npz\n",
    "In this numpy file various training indicators are stored. They are divided into 4 variables which store a vector of various metrics at each epoch.\n",
    "#### ae_training_status\n",
    "This variable stores 4 quantities per epoch: \n",
    "- epoch\n",
    "- training time\n",
    "- training loss\n",
    "- validation loss\n",
    "\n",
    "#### main_training_status\n",
    "This variable stores 16 quantities per epoch:\n",
    "- epoch\n",
    "- training time\n",
    "- discriminator loss\n",
    "- generator loss\n",
    "- validation accuracy (batch mode)\n",
    "- validation AUC (batch mode)\n",
    "- validation accuracy (single mode)\n",
    "- validation AUC (single mode)\n",
    "- training accuracy (batch mode)\n",
    "- training AUC (batch mode)\n",
    "- training accuracy (single mode)\n",
    "- training AUC (single mode)\n",
    "- mean discriminator value for pairs in dataset (train+valid in single mode)\n",
    "- mean discriminator value for pairs not in dataset (single mode)\n",
    "- mean discriminator value for pairs in dataset (train+valid in batch mode)\n",
    "- mean discriminator value for pairs not in dataset (batch mode)\n",
    "\n",
    "#### quality_status\n",
    "This variable stores 11 quantities related to the discriminator performance. Unscaled in this context means, that the decision boundary is drawn at 0.5 ([0,0.5) are false pairs, [0.5,1.0] are real pairs) and scaled uses the best performing boundary. The quality score is defined as $\\sqrt{(Sensitivity-1)^2+(Specificity-1)^2}$, zero being an indicator for good performance.\n",
    "The stored quantities are:\n",
    "- number of correctly classified training pairs (unscaled)\n",
    "- number of correctly classified validation pairs (unscaled)\n",
    "- number of falsely classified of pairs not in dataset (unscaled)\n",
    "- quality score training (unscaled)\n",
    "- quality score validation (unsclaed)\n",
    "- number of correctly classified training pairs (scaled)\n",
    "- number of correctly classified validation pairs (scaled)\n",
    "- number of falsely classified of pairs not in dataset (scaled)\n",
    "- quality score training (scaled)\n",
    "- quality score validation (scaled)\n",
    "- best boundary (used for scaled classification)\n",
    "\n",
    "\n",
    "#### generator_training_status\n",
    "This varible stores two quantities related to the generator output:\n",
    "- number of valid generator samples during the epoch. Valid means, that the rounded output corresponed to a two-hot vector\n",
    "- number of unique unique valid generator samples during the epoch\n",
    "***\n",
    "### train_ind.npy and valid_ind.npy\n",
    "The two matrix array contain the division of the main training dataset into training and validation sets. For example, with data[valid_ind], the validation data can be retrieved.\n",
    "***\n",
    "### Tensorflow model files\n",
    "The remainder of the files in the folder are the model files saved by tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('trainings/training_test/test_training_stats.npz')\n",
    "quality_status = data['quality_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
